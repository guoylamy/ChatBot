{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLEFunpdhlzL"
      },
      "source": [
        "# 1. Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmKOyIX0dBN3"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw0TQGqWh_cY"
      },
      "source": [
        "## 1.1 Data Collection \n",
        "\n",
        "Download the datasets from [Cornell Movie Datasets Website](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and unzip the data into txt files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ08H1wEiHzD",
        "outputId": "be65c0b5-32b9-4422-8c18-bef0192cbf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-26 16:21:30--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  12.5MB/s    in 0.8s    \n",
            "\n",
            "2022-04-26 16:21:31 (12.5 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n",
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "   creating: cornell movie-dialogs corpus/\n",
            "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
          ]
        }
      ],
      "source": [
        "! wget -nc \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "! unzip cornell_movie_dialogs_corpus.zip\n",
        "! rm cornell_movie_dialogs_corpus.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwM5YMC1QXH_"
      },
      "source": [
        "## 1.2 Data Cleaning & Wrangling\n",
        "\n",
        "Clean the data and convert it into the form of a dialog. We only care about \"movie_lines.txt\" and \"movie_conversations.txt\". The former one consists of [Line Number, Movie ID, Line Contents] and the latter one is comprised of all the conversations(i.e. lists of line numbers).\n",
        "\n",
        "e.g.:  \n",
        "\n",
        "L1045 +++\\$+++ u0 +++\\$+++ m0 +++\\$+++ BIANCA +++\\$+++ They do not!\n",
        "\n",
        "u0 +++\\$+++ u2 +++\\$+++ m0 +++\\$+++ ['L194', 'L195', 'L196', 'L197']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbp11GwnmtYG"
      },
      "outputs": [],
      "source": [
        "# open dialog files\n",
        "movie_lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n",
        "movie_conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaSnbTgHnnwH"
      },
      "outputs": [],
      "source": [
        "# build a dictionary to record (line_number, dialog) mappings\n",
        "line_to_dialog = {}\n",
        "for line in movie_lines:\n",
        "  line_splited = line.split(' +++$+++ ')\n",
        "  line_to_dialog[line_splited[0]] = line_splited[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktu9maMdp5YE"
      },
      "outputs": [],
      "source": [
        "# build dialog fragments\n",
        "dialog_fragments = []\n",
        "for conversation in movie_conversations:\n",
        "  ## convert u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197'] to 'L194', 'L195', 'L196', 'L197'\n",
        "  dialog_instance = conversation.split(' +++$+++ ')[-1][1:-1]\n",
        "  ## convert 'L194', 'L195', 'L196', 'L197' to ['L194', 'L195', 'L196', 'L197']\n",
        "  dialog_fragments.append(dialog_instance.replace(\"'\", \" \").replace(\",\",\"\").split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6ZyUbPJk6xE"
      },
      "outputs": [],
      "source": [
        "# convert dialog fragments into (question, answer) pairs\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for frag in dialog_fragments:\n",
        "    for i in range(1, len(frag)):\n",
        "        questions.append(line_to_dialog[frag[i-1]])\n",
        "        answers.append(line_to_dialog[frag[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAWPaVbtmdX4",
        "outputId": "d06d0645-8351-45e3-e1de-cdb27a1baa67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "\n",
            "Dialog B: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Dialog B: Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Dialog B: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: You're asking me out.  That's so cute. What's your name again?\n",
            "\n",
            "Dialog B: Forget it.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: No, no, it's my fault -- we didn't have a proper introduction ---\n",
            "\n",
            "Dialog B: Cameron.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show questions and answers pairs\n",
        "def qa_show(num):\n",
        "  for i in range(num):\n",
        "    print('-------------------------------------------------\\n')\n",
        "    print(f\"Dialog A: {questions[i]}\\n\")\n",
        "    print(f\"Dialog B: {answers[i]}\\n\")\n",
        "\n",
        "qa_show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixNAnkQiQ6wy"
      },
      "source": [
        "## 1.3 Text Preprocessing\n",
        "\n",
        "After we clean the datasets, we need to further process the texts of each question and answer. We also filter out some long questions and answers(not friendly for training the model).\n",
        "\n",
        "Set the global variables here\n",
        "\n",
        "* TEXT_LIMIT: only keep the questions and answers whose number of words is no more than `TEXT_LIMIT`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JizNt6ACLN"
      },
      "source": [
        "### 1.3.1 Filter out long Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkeekSzCSEfn"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TEXT_LIMIT = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WZ0gkXaSNNK"
      },
      "outputs": [],
      "source": [
        "# filter out long dialogs\n",
        "def filter_long_texts(questions, answers, limit):\n",
        "    short_questions = []\n",
        "    short_answers = []\n",
        "    for i in range(len(questions)):\n",
        "        if len(questions[i]) <= TEXT_LIMIT and len(answers[i]) <=TEXT_LIMIT:\n",
        "        # if len(questions[i].split()) <= TEXT_LIMIT:\n",
        "            short_questions.append(questions[i])\n",
        "            short_answers.append(answers[i])\n",
        "    return short_questions, short_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix7oU7nabaun"
      },
      "outputs": [],
      "source": [
        "filtered_questions, filtered_answers = filter_long_texts(questions, answers, TEXT_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGo3wvo0_-v5"
      },
      "source": [
        "### 1.3.2 Process the texts\n",
        "\n",
        "1. Make all the text lower case\n",
        "2. Remove contractions (e.g. she's -> she is, they're -> they are, etc.).\n",
        "3. Remove the punctuation (e.g. !, ?, $, %, #, @, ^, etc.).\n",
        "4. Tokenization.\n",
        "5. Pad the sequences to be the same length(defined as TEXT_LIMIT above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY6jluMCSsn8"
      },
      "outputs": [],
      "source": [
        "# clean the texts\n",
        "\n",
        "replacement_patterns = [\n",
        "  (r'won\\'t', 'will not'),\n",
        "  (r'can\\'t', 'cannot'),\n",
        "  (r'i\\'m', 'i am'),\n",
        "  (r'ain\\'t', 'is not'),\n",
        "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
        "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
        "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
        "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "  (r'(\\w+)\\'d', '\\g<1> would'),\n",
        "]\n",
        "\n",
        "class TextCleaner(object):\n",
        "  def __init__(self, patterns=replacement_patterns):\n",
        "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
        "    \n",
        "  def replace(self, text):\n",
        "    s = text\n",
        "    for (pattern, replace) in self.patterns:\n",
        "      s = re.sub(pattern, replace, s)\n",
        "    return s\n",
        "\n",
        "cleaner = TextCleaner()\n",
        "\n",
        "# Function for preprocessing the given text\n",
        "def preprocess_text(text):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    text = text.lower() \n",
        "    # Decontracting the text (e.g. it's -> it is)\n",
        "    text = cleaner.replace(text)\n",
        "    # Remove the punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"[ ]+\", \" \", text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcPRHsfIXDtc",
        "outputId": "cc3866af-e74f-4fc1-8910-1a3b48fdd4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: there \n",
            "\n",
            "Dialog B: Where?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: wow\n",
            "\n",
            "Dialog B: Let's go.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: she okay \n",
            "\n",
            "Dialog B: I hope so.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: they do to \n",
            "\n",
            "Dialog B: They do not!\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: who \n",
            "\n",
            "Dialog B: Joey.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: it is more\n",
            "\n",
            "Dialog B: Expensive?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: let go \n",
            "\n",
            "Dialog B: You set me up.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: thirty two \n",
            "\n",
            "Dialog B: Get out!\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: she kissed me \n",
            "\n",
            "Dialog B: Where?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: where ya goin \n",
            "\n",
            "Dialog B: Away.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: hey \n",
            "\n",
            "Dialog B: Are you lost?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: what would he say \n",
            "\n",
            "Dialog B: Who cares?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: okay \n",
            "\n",
            "Dialog B: I'm fine. I'm\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: sure i do\n",
            "\n",
            "Dialog B: Why?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: who \n",
            "\n",
            "Dialog B: Dorsey.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: dorsey \n",
            "\n",
            "Dialog B: I hate him.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: kat wake up \n",
            "\n",
            "Dialog B: What?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: maybe \n",
            "\n",
            "Dialog B: No, you weren't\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: do what \n",
            "\n",
            "Dialog B: This.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: who \n",
            "\n",
            "Dialog B: BIANCA\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you re so \n",
            "\n",
            "Dialog B: Pleasant?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: wholesome \n",
            "\n",
            "Dialog B: Unwelcome.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you up for it \n",
            "\n",
            "Dialog B: For. . . ?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: state trooper \n",
            "\n",
            "Dialog B: Fallacy.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: fallacy \n",
            "\n",
            "Dialog B: The duck?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: the duck \n",
            "\n",
            "Dialog B: Hearsay.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: no \n",
            "\n",
            "Dialog B: No what?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: parts of it \n",
            "\n",
            "Dialog B: Which parts?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you think \n",
            "\n",
            "Dialog B: Oh yeah.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: macbeth right \n",
            "\n",
            "Dialog B: Right.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: right \n",
            "\n",
            "Dialog B: Kat a fan, too?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: kat a fan too \n",
            "\n",
            "Dialog B: Yeah...\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: say it\n",
            "\n",
            "Dialog B: What?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: what \n",
            "\n",
            "Dialog B: Good enough.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: she said yes \n",
            "\n",
            "Dialog B: Thank God...\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: no \n",
            "\n",
            "Dialog B: No?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: thousands \n",
            "\n",
            "Dialog B: Why?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: no no way \n",
            "\n",
            "Dialog B: But it's...\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: where is she \n",
            "\n",
            "Dialog B: Takin' a bath.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: takin a bath \n",
            "\n",
            "Dialog B: Any I.D.?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: are you hit \n",
            "\n",
            "Dialog B: No. I'm okay.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: pretty \n",
            "\n",
            "Dialog B: Hmmmm.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you are polish \n",
            "\n",
            "Dialog B: My folks are.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: my folks are \n",
            "\n",
            "Dialog B: Stay here.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you thirsty \n",
            "\n",
            "Dialog B: I'm on duty.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: alright \n",
            "\n",
            "Dialog B: Alright?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: alright \n",
            "\n",
            "Dialog B: Alright.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: alright \n",
            "\n",
            "Dialog B: Alright.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: alright \n",
            "\n",
            "Dialog B: Okay.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: hey honey \n",
            "\n",
            "Dialog B: Hey.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_questions = [preprocess_text(que) for que in filtered_questions]\n",
        "cleaned_answers = [' '.join(ans.split()[:TEXT_LIMIT-2]) for ans in filtered_answers]\n",
        "\n",
        "for i in range(50):\n",
        "    print('-------------------------------------------------\\n')\n",
        "    print(f'Dialog A: {cleaned_questions[i]}\\n')\n",
        "    print(f'Dialog B: {cleaned_answers[i]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEuvFeGuF9bq"
      },
      "source": [
        "### 1.3.3 Add Tag Tokens\n",
        "\n",
        "After preprocessing the dataset, we should add a start tag (e.g. `<start>`) and an end tag (e.g. `<end>`) to answers. Remember that we will only add these tags to answers and not questions.\n",
        "\n",
        "When Seq2Seq model is generating the word answers, we can first send it the `<start>` to begin the word generation. When `<end>` is generated, we will stop the iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETc_LE9sXX0O"
      },
      "outputs": [],
      "source": [
        "cleaned_answers = [\"starttoken \" + ans + \" endtoken\" for ans in cleaned_answers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-_gr3HMCJ1G"
      },
      "source": [
        "### 1.3.4 Truncate the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lakxrfs9H0uQ"
      },
      "outputs": [],
      "source": [
        "# trim the data in case of running out of RAM\n",
        "\n",
        "# TRAINING_SIZE = 100000\n",
        "\n",
        "## Version 1: Take all data\n",
        "training_questions = cleaned_questions\n",
        "training_answers = cleaned_answers\n",
        "\n",
        "# Version 1: Take first TRAINING_SIZE data\n",
        "# training_questions = cleaned_questions[:TRAINING_SIZE]\n",
        "# training_answers = cleaned_answers[:TRAINING_SIZE]\n",
        "\n",
        "# testing_questions = cleaned_questions[TRAINING_SIZE:]\n",
        "# testing_answers = cleaned_answers[TRAINING_SIZE:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj2VSg6ento0"
      },
      "source": [
        "## 1.4 Input Encoding\n",
        "\n",
        "Since String inputs cannot been directly fed into our RNN model, we have to convet String input into numerical values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-JpUHtbDCkg"
      },
      "source": [
        "### 1.4.1 Choose Vocabulary Size \n",
        "\n",
        "The size of vocabulary based on the training datasets will be very large. Some of them only appears once or twice. We want to keep the most common words while maitain the diversity of expressions, so we need to figure out how many words to keep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASDnhWKnFrMa"
      },
      "source": [
        "First, figure out the frequence of each words in the questions and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbguHoq1FDyx"
      },
      "outputs": [],
      "source": [
        "word2frequency = {}\n",
        "for sent in cleaned_questions + cleaned_answers:\n",
        "    for word in sent.split():\n",
        "        if word in word2frequency:\n",
        "            word2frequency[word] += 1\n",
        "        else:\n",
        "            word2frequency[word] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_vIQgDVGXq9",
        "outputId": "71424ea7-881d-401c-db6f-90d6a9677e10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7799, 2540, 1518, 1108, 890, 752, 664, 585, 525, 469]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "FREQUENCY_THRESHOLD = 10\n",
        "cumulative_num_words_by_threshold = [0 for i in range(FREQUENCY_THRESHOLD)]\n",
        "\n",
        "for word in word2frequency:\n",
        "    for i in range(1, FREQUENCY_THRESHOLD+1):\n",
        "        if word2frequency[word] >= i:\n",
        "            cumulative_num_words_by_threshold[i-1] += 1\n",
        "        else:\n",
        "            break\n",
        "cumulative_num_words_by_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1hB8VzhGKJd"
      },
      "source": [
        "Then, visualize the Word Frequency "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hBKBGTtrIugc",
        "outputId": "3a528cde-c71e-482e-f747-b48320467a1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "X = [i+1 for i in range(FREQUENCY_THRESHOLD)]\n",
        "\n",
        "# bar plot the data\n",
        "plt.bar(X, height=cumulative_num_words_by_threshold)\n",
        "\n",
        "# add value to the top of the bar, making the plot more clear\n",
        "for index,data in enumerate(cumulative_num_words_by_threshold):\n",
        "    plt.text(x=index+0.55, y =data+100 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
        "\n",
        "# add plot information\n",
        "plt.xticks(X, [str(i) for i in X])\n",
        "plt.xlabel(\"Thresholds\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "plt.title(\"Number of Words(freq >= threshold) vs Thresholds\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMcerLyFBmq"
      },
      "source": [
        "Set global variables here\n",
        "\n",
        "* NUM_WORDS: Only keep the NUM_WORDS most common words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJUf6CjJ37u7"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = 3500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuOIlJnJMnTi"
      },
      "source": [
        "### 1.4.2 Build the Vocabulary\n",
        "\n",
        "The words in vocabulary should all come from training datasets. In order to train a good RNN model, we have to discard less common words and try different `NUM_WORDS` to see we is best for our datasets.\n",
        "\n",
        "* Set the Tokenizer to only keep the NUM_WORDS most common words\n",
        "* Set the less frequent words as 'nulltoken' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjSxe5uGIH6q"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words = NUM_WORDS, oov_token='nulltoken')\n",
        "\n",
        "# Fit the tokenizer to questions and answers\n",
        "tokenizer.fit_on_texts(training_answers + training_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTAwFiL9Wq3",
        "outputId": "b9b4bf6d-383c-4cba-abff-c6c5ab775143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "tokenizer.word_index[\"endtoken\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmb6SWKyQwm1"
      },
      "source": [
        "### 1.4.3 Prepare the Encoder Input\n",
        "\n",
        "Since we have successfully built the vocabulary, each word in the vocabulary will corrspond to a unique index according to their frequency. Now convert a question String input into a list of integer as Encoder part of the LSTM network. Later, we can feed the integer values to our model and the predicted results will also be an integer and then we can convert it back to the word the integer number corresponds to based on the vocabulary we build. \n",
        "\n",
        "The preparation includes:\n",
        "\n",
        "* Convert words in questions to corresponding numerical index\n",
        "* Pad zeros to the end of short questions to make sure all inputs have the same length(TEXT_LIMIT)\n",
        "* Convert the list of integers into a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCeEQiMwIZas",
        "outputId": "4916fe00-21bd-45c2-cfa9-a13cf08a9200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9905, 15) 15\n"
          ]
        }
      ],
      "source": [
        "### encoder input data\n",
        "\n",
        "# Tokenize the questions\n",
        "tokenized_questions_training = tokenizer.texts_to_sequences(training_questions)\n",
        "# tokenized_questions_testing = tokenizer.texts_to_sequences(testing_questions)\n",
        "\n",
        "# Pad the sequences\n",
        "padded_questions_training = pad_sequences(tokenized_questions_training, maxlen=TEXT_LIMIT, padding='post')\n",
        "# padded_questions_testing = pad_sequences(tokenized_questions_testing, maxlen=TEXT_LIMIT, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "encoder_input_data_training = np.array(padded_questions_training)\n",
        "print(encoder_input_data_training.shape, TEXT_LIMIT)\n",
        "# encoder_input_data_testing = np.array(padded_questions_testing)\n",
        "# print(encoder_input_data_testing.shape, TEXT_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FolI3DmgR8c9"
      },
      "source": [
        "### 1.4.4 Prepare the Decoder Input\n",
        "\n",
        "Similarly, what we have done above. Now we convert the answer String inputs to a list of integers as our decoder input.\n",
        "\n",
        "The preparation includes:\n",
        "\n",
        "* Convert words in answers to corresponding numerical index\n",
        "* Pad zeros to the end of short answers to make sure all answers have the same length(TEXT_LIMIT)\n",
        "* Convert the list of integers into a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoKg2PHoIkcH",
        "outputId": "826ea7ae-ca4c-4708-b210-e345837d4a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9905, 15) 15\n"
          ]
        }
      ],
      "source": [
        "### decoder input data\n",
        "\n",
        "# Tokenize the answers\n",
        "tokenized_answers_training = tokenizer.texts_to_sequences(training_answers)\n",
        "# tokenized_answers_testing = tokenizer.texts_to_sequences(testing_answers)\n",
        "\n",
        "# Pad the sequences\n",
        "padded_answers_training = pad_sequences(tokenized_answers_training, maxlen=TEXT_LIMIT, padding='post')\n",
        "# padded_answers_testing = pad_sequences(tokenized_answers_testing, maxlen=TEXT_LIMIT, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "decoder_input_data_training = np.array(padded_answers_training)\n",
        "print(decoder_input_data_training.shape, TEXT_LIMIT)\n",
        "\n",
        "# decoder_input_data_testing = np.array(padded_answers_testing)\n",
        "# print(decoder_input_data_testing.shape, TEXT_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs7PPadHTbkN"
      },
      "source": [
        "### 1.4.5 Prepare the Output Labels\n",
        "\n",
        " The Chatbot training can be considered as a classification task. Therefore, we use `to_categorical` to converts class vectors (integers) to binary class matrixs as the training labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXi3CTOMI8AX"
      },
      "outputs": [],
      "source": [
        "### decoder_output_data\n",
        "\n",
        "# Iterate through index of tokenized answers\n",
        "for i in range(len(tokenized_answers_training)):\n",
        "    tokenized_answers_training[i] = tokenized_answers_training[i][1:]\n",
        "\n",
        "# for i in range(len(tokenized_answers_testing)):\n",
        "#     tokenized_answers_testing[i] = tokenized_answers_testing[i][1:]\n",
        "\n",
        "# Pad the tokenized answers\n",
        "padded_answers_training = pad_sequences(tokenized_answers_training, maxlen = TEXT_LIMIT, padding = 'post')\n",
        "# padded_answers_testing = pad_sequences(tokenized_answers_testing, maxlen = TEXT_LIMIT, padding = 'post')\n",
        "\n",
        "# One hot encode\n",
        "onehot_answers_training = utils.to_categorical(padded_answers_training, NUM_WORDS+1)\n",
        "# onehot_answers_testing = utils.to_categorical(padded_answers_testing, NUM_WORDS+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnyhJ7VC4-YY",
        "outputId": "d9271803-741a-4ea2-b169-56f5789e8f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9905, 15, 3501)\n"
          ]
        }
      ],
      "source": [
        "# Convert to numpy array\n",
        "decoder_output_data_training = np.array(onehot_answers_training)\n",
        "del(onehot_answers_training)\n",
        "\n",
        "# decoder_output_data_testing = np.array(onehot_answers_testing)\n",
        "# del (onehot_answers_testing)\n",
        "\n",
        "print(decoder_output_data_training.shape)\n",
        "# print(decoder_output_data_testing.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJzOooGr6cea"
      },
      "source": [
        "# 2. Training RNN Network\n",
        "\n",
        "In this section, we build a Sequence to Sequence(or Seq2Seq), which includes a Encoder Part and Decoder Part. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefAip_h7cRZ"
      },
      "source": [
        "Check the number of GPUs working in the devices. Different from Pytorch, Tensorflow will automatically utilize GPUs resources first and doesn't need to change the codes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU5-frv8583-",
        "outputId": "c314f4fc-5994-446f-cc10-f9a2d2d7a885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "## version 1.x\n",
        "# print(\"Num GPUs Available: \", tf.config.experimental_list_devices())\n",
        "\n",
        "# verison 2.x\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TinhJ0ycgJgW"
      },
      "source": [
        "## 2.1 Build the Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0YZ_hSxgQ7p"
      },
      "source": [
        "### 2.1.1 Hyperparameter Configuration\n",
        "\n",
        "Set hyper-parameters for Training Model:\n",
        "\n",
        "* BATCH_SIZE: Usually a multiple of 2 and no larger than 64\n",
        "* EPOCHS: number of epochs for training the model\n",
        "* VOCAB_SIZE: Dimension of categories. It equals to NUM_WORDS + 1 due to `nulltokens`\n",
        "* embed_dim: Output dimension of embedding layers\n",
        "* num_lstm: number of units used in LSTM layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMch0EsnkmMN"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 150\n",
        "VOCAB_SIZE = NUM_WORDS + 1\n",
        "\n",
        "embed_dim = 32\n",
        "num_lstm = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2BojtyLgH0h"
      },
      "source": [
        "### 2.1.2 Build the Encoder part\n",
        "\n",
        "Parse question input vectors into an Embedding layer, set the input size as (1 + Number of common words kept) where 1 is used for `nulltoken`. Feed the embedding into the LSTM model and keep track of output states and hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwi7M2Nmk1oR"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- 1 layer ---------------------------------------------------------\n",
        "# # Input for encoder\n",
        "# encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
        "\n",
        "# # Embedding layer\n",
        "# encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
        "\n",
        "# # LSTM layer (that returns states in addition to output)\n",
        "# encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
        "\n",
        "# # Get the states for encoder\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# --------------------------------------------------- 2 layers ---------------------------------------------------------\n",
        "# Input for encoder\n",
        "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
        "\n",
        "# Embedding layer\n",
        "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
        "\n",
        "# LSTM layer (that returns states in addition to output)\n",
        "encoder_lstm_output1, h1, c1 = LSTM(units = num_lstm, return_sequences=True, return_state = True, name='encoder_lstm_layer1', recurrent_dropout=0.3)(encoder_embedding)\n",
        "encoder_outputs, h2, c2 = LSTM(units = num_lstm, return_state = True, name='encoder_lstm_layer2')(encoder_lstm_output1)\n",
        "\n",
        "# Get the states for encoder\n",
        "encoder_states = [h1, c1, h2, c2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKjvmqCaiHgr"
      },
      "source": [
        "### 2.1.3 Build the Decoder part\n",
        "\n",
        "Parse answer input vectors into an Embedding layer similar to Encoder part, feed the embedding together with encoder output stats and hidden states into decoder LSTM layer. A dense layer is connected to the LSTM and `softmax` is used to do the final classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbyKdEcXk6d6"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- 1 layer ---------------------------------------------------------\n",
        "# # Input for decoder\n",
        "# decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
        "\n",
        "# # Embedding layer\n",
        "# decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
        "\n",
        "# # LSTM layer (that returns states and sequences as well)\n",
        "# decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
        "\n",
        "# # Get the output of LSTM layer, using the initial states from the encoder\n",
        "# decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
        "\n",
        "# # Dense layer\n",
        "# decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='decoder_outputs') \n",
        "\n",
        "# # Get the output of Dense layer\n",
        "# output = decoder_dense(decoder_outputs)\n",
        "\n",
        "# --------------------------------------------------- 2 layers ---------------------------------------------------------\n",
        "# Input for decoder\n",
        "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
        "\n",
        "# Embedding layer\n",
        "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
        "\n",
        "# LSTM layer (that returns states and sequences as well)\n",
        "# Get the output of LSTM layer, using the initial states from the encoder\n",
        "decoder_lstm_layer1 = LSTM(units = num_lstm, return_sequences=True, return_state = True, name='decoder_lstm_layer1', recurrent_dropout=0.3)\n",
        "decoder_lstm_output1, dh1, dc1 = decoder_lstm_layer1(inputs = decoder_embedding, initial_state = [h1, c1])\n",
        "\n",
        "decoder_lstm_layer2 = LSTM(units = num_lstm , return_state = True, return_sequences = True, name='decoder_lstm_layer2')\n",
        "decoder_outputs, dh2, dc2  = decoder_lstm_layer2(inputs = decoder_lstm_output1, initial_state = [h2, c2])\n",
        "# Dense layer\n",
        "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='decoder_outputs') \n",
        "\n",
        "# Get the output of Dense layer\n",
        "output = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxJHpP3Mi-nu"
      },
      "source": [
        "### 2.1.4 Set Model Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJHivVPEjQ9g"
      },
      "source": [
        "Concatenate the encoder and decoder parts of the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK6yhoCtlDYK"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCIVkwL_jVnp"
      },
      "source": [
        "Choose `adam` as our model, a good tip for training the RNN network is to use adaptive learning rate optimizer such as `adam`. Set metrics, so that we can see accuracy and loss for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_nMCQ3VlHoO"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', metrics=['acc'], loss = \"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHJ7aT6GlH4x",
        "outputId": "6e3941cd-2d6b-451b-ab82-59be8f50b98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 32)     112032      ['encoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 32)     112032      ['decoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " encoder_lstm_layer1 (LSTM)     [(None, None, 128),  82432       ['encoder_embedding[0][0]']      \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm_layer1 (LSTM)     [(None, None, 128),  82432       ['decoder_embedding[0][0]',      \n",
            "                                 (None, 128),                     'encoder_lstm_layer1[0][1]',    \n",
            "                                 (None, 128)]                     'encoder_lstm_layer1[0][2]']    \n",
            "                                                                                                  \n",
            " encoder_lstm_layer2 (LSTM)     [(None, 128),        131584      ['encoder_lstm_layer1[0][0]']    \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm_layer2 (LSTM)     [(None, None, 128),  131584      ['decoder_lstm_layer1[0][0]',    \n",
            "                                 (None, 128),                     'encoder_lstm_layer2[0][1]',    \n",
            "                                 (None, 128)]                     'encoder_lstm_layer2[0][2]']    \n",
            "                                                                                                  \n",
            " decoder_outputs (Dense)        (None, None, 3501)   451629      ['decoder_lstm_layer2[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,103,725\n",
            "Trainable params: 1,103,725\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9qkcHCqiVIM"
      },
      "source": [
        "### 2.1.4 Checkpoint Callback\n",
        "\n",
        "Since we train the model in the AWS sagemaker(or Colab), setting the checkpoint is a good practice to save the model in case of network issues or accidental interrupts.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wb9Vj-ji21t"
      },
      "outputs": [],
      "source": [
        "! mkdir -p ./ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ow0USdd583_"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = './ckpt/checkpoints-{epoch:03d}.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1_GchH0ixWi"
      },
      "source": [
        "## 2.2 Train the LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nakcTQc3kJnt"
      },
      "source": [
        "### 2.2.1 Fit the Model\n",
        "\n",
        "* Feed in the preprocessed data\n",
        "* Set defined hyperparameters\n",
        "* shuffle the datasets for each epoch such that each batch of data can better representate the whole datasets.\n",
        "* split the input data into 90% training data and 10% validation data\n",
        "* weights will be saved when each epoch finishes  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfVO8dDhlNeH",
        "outputId": "49ebdd01-e8df-4734-a9c2-0c2736eba719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101/150\n",
            "279/279 [==============================] - 73s 209ms/step - loss: 0.1735 - acc: 0.8250 - val_loss: 1.2672 - val_acc: 0.5209\n",
            "Epoch 102/150\n",
            "279/279 [==============================] - 56s 200ms/step - loss: 0.1666 - acc: 0.8282 - val_loss: 1.2652 - val_acc: 0.5201\n",
            "Epoch 103/150\n",
            "279/279 [==============================] - 55s 198ms/step - loss: 0.1648 - acc: 0.8281 - val_loss: 1.2723 - val_acc: 0.5190\n",
            "Epoch 104/150\n",
            "279/279 [==============================] - 55s 197ms/step - loss: 0.1636 - acc: 0.8291 - val_loss: 1.2705 - val_acc: 0.5225\n",
            "Epoch 105/150\n",
            "279/279 [==============================] - 55s 195ms/step - loss: 0.1629 - acc: 0.8286 - val_loss: 1.2768 - val_acc: 0.5161\n",
            "Epoch 106/150\n",
            "279/279 [==============================] - 55s 196ms/step - loss: 0.1614 - acc: 0.8297 - val_loss: 1.2832 - val_acc: 0.5203\n",
            "Epoch 107/150\n",
            "279/279 [==============================] - 55s 197ms/step - loss: 0.1603 - acc: 0.8310 - val_loss: 1.2916 - val_acc: 0.5177\n",
            "Epoch 108/150\n",
            "279/279 [==============================] - 56s 199ms/step - loss: 0.1596 - acc: 0.8312 - val_loss: 1.2953 - val_acc: 0.5195\n",
            "Epoch 109/150\n",
            "279/279 [==============================] - 56s 199ms/step - loss: 0.1583 - acc: 0.8311 - val_loss: 1.2962 - val_acc: 0.5190\n",
            "Epoch 110/150\n",
            "279/279 [==============================] - 55s 199ms/step - loss: 0.1576 - acc: 0.8322 - val_loss: 1.2996 - val_acc: 0.5225\n",
            "Epoch 111/150\n",
            "279/279 [==============================] - 54s 195ms/step - loss: 0.1563 - acc: 0.8321 - val_loss: 1.2993 - val_acc: 0.5169\n",
            "Epoch 112/150\n",
            "279/279 [==============================] - 54s 195ms/step - loss: 0.1558 - acc: 0.8328 - val_loss: 1.3060 - val_acc: 0.5164\n",
            "Epoch 113/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1548 - acc: 0.8336 - val_loss: 1.3085 - val_acc: 0.5225\n",
            "Epoch 114/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1544 - acc: 0.8344 - val_loss: 1.3052 - val_acc: 0.5227\n",
            "Epoch 115/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1533 - acc: 0.8338 - val_loss: 1.3130 - val_acc: 0.5190\n",
            "Epoch 116/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1528 - acc: 0.8339 - val_loss: 1.3185 - val_acc: 0.5195\n",
            "Epoch 117/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1520 - acc: 0.8353 - val_loss: 1.3226 - val_acc: 0.5177\n",
            "Epoch 118/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1512 - acc: 0.8364 - val_loss: 1.3229 - val_acc: 0.5179\n",
            "Epoch 119/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1506 - acc: 0.8359 - val_loss: 1.3254 - val_acc: 0.5193\n",
            "Epoch 120/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1499 - acc: 0.8367 - val_loss: 1.3255 - val_acc: 0.5201\n",
            "Epoch 121/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1488 - acc: 0.8379 - val_loss: 1.3298 - val_acc: 0.5190\n",
            "Epoch 122/150\n",
            "279/279 [==============================] - 54s 192ms/step - loss: 0.1486 - acc: 0.8356 - val_loss: 1.3318 - val_acc: 0.5177\n",
            "Epoch 123/150\n",
            "279/279 [==============================] - 54s 192ms/step - loss: 0.1484 - acc: 0.8380 - val_loss: 1.3426 - val_acc: 0.5150\n",
            "Epoch 124/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1475 - acc: 0.8359 - val_loss: 1.3393 - val_acc: 0.5190\n",
            "Epoch 125/150\n",
            "279/279 [==============================] - 54s 192ms/step - loss: 0.1466 - acc: 0.8374 - val_loss: 1.3433 - val_acc: 0.5201\n",
            "Epoch 126/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1465 - acc: 0.8367 - val_loss: 1.3457 - val_acc: 0.5214\n",
            "Epoch 127/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1460 - acc: 0.8356 - val_loss: 1.3549 - val_acc: 0.5121\n",
            "Epoch 128/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1453 - acc: 0.8386 - val_loss: 1.3522 - val_acc: 0.5182\n",
            "Epoch 129/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1449 - acc: 0.8394 - val_loss: 1.3551 - val_acc: 0.5182\n",
            "Epoch 130/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1444 - acc: 0.8367 - val_loss: 1.3539 - val_acc: 0.5145\n",
            "Epoch 131/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1442 - acc: 0.8385 - val_loss: 1.3532 - val_acc: 0.5190\n",
            "Epoch 132/150\n",
            "279/279 [==============================] - 55s 197ms/step - loss: 0.1438 - acc: 0.8389 - val_loss: 1.3590 - val_acc: 0.5169\n",
            "Epoch 133/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1434 - acc: 0.8392 - val_loss: 1.3594 - val_acc: 0.5177\n",
            "Epoch 134/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1426 - acc: 0.8399 - val_loss: 1.3626 - val_acc: 0.5134\n",
            "Epoch 135/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1422 - acc: 0.8398 - val_loss: 1.3705 - val_acc: 0.5182\n",
            "Epoch 136/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1416 - acc: 0.8400 - val_loss: 1.3724 - val_acc: 0.5142\n",
            "Epoch 137/150\n",
            "279/279 [==============================] - 53s 192ms/step - loss: 0.1412 - acc: 0.8400 - val_loss: 1.3737 - val_acc: 0.5169\n",
            "Epoch 138/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1410 - acc: 0.8397 - val_loss: 1.3737 - val_acc: 0.5158\n",
            "Epoch 139/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1408 - acc: 0.8384 - val_loss: 1.3722 - val_acc: 0.5169\n",
            "Epoch 140/150\n",
            "279/279 [==============================] - 54s 193ms/step - loss: 0.1404 - acc: 0.8402 - val_loss: 1.3738 - val_acc: 0.5140\n",
            "Epoch 141/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1396 - acc: 0.8409 - val_loss: 1.3784 - val_acc: 0.5179\n",
            "Epoch 142/150\n",
            "279/279 [==============================] - 54s 195ms/step - loss: 0.1394 - acc: 0.8409 - val_loss: 1.3801 - val_acc: 0.5179\n",
            "Epoch 143/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1393 - acc: 0.8413 - val_loss: 1.3781 - val_acc: 0.5156\n",
            "Epoch 144/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1391 - acc: 0.8407 - val_loss: 1.3846 - val_acc: 0.5166\n",
            "Epoch 145/150\n",
            "279/279 [==============================] - 54s 195ms/step - loss: 0.1382 - acc: 0.8421 - val_loss: 1.3859 - val_acc: 0.5190\n",
            "Epoch 146/150\n",
            "279/279 [==============================] - 54s 192ms/step - loss: 0.1385 - acc: 0.8412 - val_loss: 1.3938 - val_acc: 0.5145\n",
            "Epoch 147/150\n",
            "279/279 [==============================] - 53s 192ms/step - loss: 0.1378 - acc: 0.8423 - val_loss: 1.3890 - val_acc: 0.5195\n",
            "Epoch 148/150\n",
            "279/279 [==============================] - 54s 192ms/step - loss: 0.1377 - acc: 0.8416 - val_loss: 1.3944 - val_acc: 0.5177\n",
            "Epoch 149/150\n",
            "279/279 [==============================] - 54s 194ms/step - loss: 0.1374 - acc: 0.8402 - val_loss: 1.3912 - val_acc: 0.5190\n",
            "Epoch 150/150\n",
            "279/279 [==============================] - 55s 196ms/step - loss: 0.1372 - acc: 0.8406 - val_loss: 1.3948 - val_acc: 0.5190\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f404246aa50>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    x = {\"encoder_inputs\": encoder_input_data_training, \"decoder_inputs\": decoder_input_data_training}, \n",
        "    y = {'decoder_outputs': decoder_output_data_training}, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epochs = EPOCHS,\n",
        "    shuffle = True,\n",
        "    validation_split = 0.1,\n",
        "    callbacks=[model_checkpoint_callback],\n",
        "    initial_epoch = 100\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7vKudyok9F0"
      },
      "source": [
        "### 2.2.2 Save the Model\n",
        "\n",
        "Save the final weights to specific filepath."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfenJUKjcbwK"
      },
      "outputs": [],
      "source": [
        "! mkdir -p model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6amEIRtjlo-w"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"./model/weight_2l32d128u.h5\")\n",
        "model.save(filepath=f\"./model/model_2l32d128u.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMtI7RRq6B_U"
      },
      "source": [
        "# 3. Inference\n",
        "\n",
        "Build the inference model(the same as training model) and load the trained weight. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reX6WFs4lgqb"
      },
      "source": [
        "## 3.1 Load the weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYVsC70Y0Sau"
      },
      "outputs": [],
      "source": [
        "# load weights\n",
        "model.load_weights(\"./model/weight_2l32d128u.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yOIxJe0ln4V"
      },
      "source": [
        "## 3.2 Build Inference Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH4GLMnk60hE"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------- 1 layer --------------------------------------------------\n",
        "# # Function for making inference\n",
        "# def make_inference_models():\n",
        "    \n",
        "#     # Create a model that takes encoder's input and outputs the states for encoder\n",
        "#     encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "    \n",
        "#     # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
        "#     decoder_state_input_h = Input(shape = (num_lstm, ), name='decoder_state_input_h')\n",
        "#     decoder_state_input_c = Input(shape = (num_lstm, ), name='decoder_state_input_c')\n",
        "    \n",
        "#     # Store the two inputs for decoder inside a list\n",
        "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "#     # Pass the inputs through LSTM layer you have created before\n",
        "#     decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
        "    \n",
        "#     # Store the outputted hidden state and cell state from LSTM inside a list\n",
        "#     decoder_states = [state_h, state_c]\n",
        "\n",
        "#     # Pass the output from LSTM layer through the dense layer you have created before\n",
        "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#     # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
        "#     decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "#                           [decoder_outputs] + decoder_states)\n",
        "    \n",
        "#     return encoder_model , decoder_model\n",
        "\n",
        "# ------------------------------------------------------- 2 layer --------------------------------------------------\n",
        "# Function for making inference\n",
        "def make_inference_models():\n",
        "    ######################\n",
        "    ### refer to: https://stackoverflow.com/questions/50915634/multilayer-seq2seq-model-with-lstm-in-keras\n",
        "    ######################\n",
        "    # Create a model that takes encoder's input and outputs the states for encoder\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "    \n",
        "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
        "    decoder_state_input_h1 = Input(shape = (num_lstm, ), name='decoder_state_input_h1')\n",
        "    decoder_state_input_c1 = Input(shape = (num_lstm, ), name='decoder_state_input_c1')\n",
        "    decoder_state_input_h2 = Input(shape = (num_lstm, ), name='decoder_state_input_h2')\n",
        "    decoder_state_input_c2 = Input(shape = (num_lstm, ), name='decoder_state_input_c2') \n",
        "    \n",
        "    # Store the two inputs for decoder inside a list\n",
        "    decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2]\n",
        "    \n",
        "    # Pass the inputs through LSTM layer you have created before\n",
        "    # decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm_layer1(decoder_embedding, initial_state=decoder_states_inputs[:2])\n",
        "    decoder_outputs, state_h1, state_c1 = decoder_lstm_layer2(decoder_outputs, initial_state=decoder_states_inputs[-2:])\n",
        "    \n",
        "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
        "    # decoder_states = [state_h, state_c]\n",
        "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
        "\n",
        "    # Pass the output from LSTM layer through the dense layer you have created before\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD5ePfvV63vc",
        "outputId": "b2905da0-0e95-40d4-eef9-fd606a7becb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 32)     112032      ['decoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " decoder_state_input_h1 (InputL  [(None, 128)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " decoder_state_input_c1 (InputL  [(None, 128)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " decoder_lstm_layer1 (LSTM)     [(None, None, 128),  82432       ['decoder_embedding[0][0]',      \n",
            "                                 (None, 128),                     'decoder_state_input_h1[0][0]', \n",
            "                                 (None, 128)]                     'decoder_state_input_c1[0][0]'] \n",
            "                                                                                                  \n",
            " decoder_state_input_h2 (InputL  [(None, 128)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " decoder_state_input_c2 (InputL  [(None, 128)]       0           []                               \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " decoder_lstm_layer2 (LSTM)     [(None, None, 128),  131584      ['decoder_lstm_layer1[1][0]',    \n",
            "                                 (None, 128),                     'decoder_state_input_h2[0][0]', \n",
            "                                 (None, 128)]                     'decoder_state_input_c2[0][0]'] \n",
            "                                                                                                  \n",
            " decoder_outputs (Dense)        (None, None, 3501)   451629      ['decoder_lstm_layer2[1][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 777,677\n",
            "Trainable params: 777,677\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model for inference\n",
        "enc_model , dec_model = make_inference_models()\n",
        "dec_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNu43IlHo4F8"
      },
      "source": [
        "## 3.3 Preprocess the Inference Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MEWuJdn7H5M"
      },
      "outputs": [],
      "source": [
        "# Function for converting strings to tokens\n",
        "def str_to_tokens(sentence: str, tokenizer, maxlen_questions=TEXT_LIMIT):\n",
        "\n",
        "    # Lowercase the sentence and split it into words\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    tokens_list = tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "    # Pad the sequences to be the same length\n",
        "    return pad_sequences(tokens_list , maxlen = maxlen_questions, padding = 'post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EPwltTkpH8w"
      },
      "source": [
        "## 3.4 Main Inference\n",
        "\n",
        "The inference follows the steps below:\n",
        "\n",
        "* read in user-entered questions and preprocess the data into the same form of training data\n",
        "* initialize an empty sequence with `starttoken`\n",
        "* keep inference until encounter a `endtoken` or sequence is too long\n",
        "* convert the predicted sequence back to words by using Vocabulary built before "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnuR5dmZ66h1",
        "outputId": "76b5b03f-bf38-4852-bbaf-d3f44537303d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : hi there\n",
            "hi\n",
            "Enter question : what's your name?\n",
            "doyle\n",
            "Enter question : nice to meet you\n",
            "yeah\n",
            "Enter question : how is it going\n",
            "fine\n",
            "Enter question : good bye\n",
            "good bye\n"
          ]
        }
      ],
      "source": [
        "#------------------------------------------------------------1 layer-------------------------------------------------------\n",
        "# # Iterate through the number of times you want to ask question\n",
        "# try:\n",
        "#     for _ in range(5):\n",
        "\n",
        "#         # Get the input and predict it with the encoder model\n",
        "#         encoder_inputs = str_to_tokens(preprocess_text(input('Enter question : ')), tokenizer)\n",
        "#         states_values = enc_model.predict(encoder_inputs)\n",
        "\n",
        "#         # Initialize the decoder input sequence with the starttoken index\n",
        "#         # Reshape this to be a (1, 1) array, since it is a sequence of 1 sample for 1 timestep\n",
        "#         empty_target_seq = np.zeros((1, 1))\n",
        "\n",
        "#         # Update the target sequence with index of \"start\"\n",
        "\n",
        "#         empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
        "#         # Initialize the stop condition with False\n",
        "#         stop_condition = False\n",
        "\n",
        "#         # Initialize the decoded words with an empty string\n",
        "#         decoded_translation = []\n",
        "\n",
        "#         # While stop_condition is false\n",
        "#         while not stop_condition :\n",
        "\n",
        "#             # Predict the (target sequence + the output from encoder model) with decoder model\n",
        "#             dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "#             # Get the index for sampled word using the dec_outputs\n",
        "#             # dec_outputs is a numpy array of the shape (sample, timesteps, VOCAB_SIZE)\n",
        "#             # To start, we can just pick the word with the higest probability - greedy search\n",
        "#             sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "            \n",
        "#             # Initialize the sampled word with None\n",
        "#             sampled_word = None\n",
        "\n",
        "#             # Iterate through words and their indexes\n",
        "#             for word, index in tokenizer.word_index.items() :\n",
        "\n",
        "#                 # If the index is equal to sampled word's index\n",
        "#                 if sampled_word_index == index :\n",
        "\n",
        "#                     # Add the word to the decoded string\n",
        "#                     decoded_translation.append(word)\n",
        "\n",
        "#                     # Update the sampled word\n",
        "#                     sampled_word = word\n",
        "\n",
        "#             # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
        "#             if sampled_word == 'endtoken' or len(decoded_translation) > TEXT_LIMIT:\n",
        "\n",
        "#                 # Make the stop_condition to true\n",
        "#                 stop_condition = True\n",
        "\n",
        "#             # Initialize back the target sequence to zero - array([[0.]])    \n",
        "#             empty_target_seq = np.zeros(shape = (1, 1))  \n",
        "\n",
        "#             # Update the target sequence with index of \"start\"\n",
        "#             empty_target_seq[0, 0] = sampled_word_index\n",
        "\n",
        "#             # Get the state values\n",
        "#             states_values = [h, c] \n",
        "\n",
        "#             # Print the decoded string\n",
        "#         print(' '.join(decoded_translation[:-1]))\n",
        "# except KeyboardInterrupt:\n",
        "#     print('Ending conversational agent')\n",
        "\n",
        "#------------------------------------------------------------ 2 layers -------------------------------------------------------\n",
        "\n",
        "# Iterate through the number of times you want to ask question\n",
        "try:\n",
        "    for _ in range(5):\n",
        "\n",
        "        # Get the input and predict it with the encoder model\n",
        "        encoder_inputs = str_to_tokens(preprocess_text(input('Enter question : ')), tokenizer)\n",
        "        states_values = enc_model.predict(encoder_inputs)\n",
        "        # print(len(states_values))\n",
        "        # Initialize the decoder input sequence with the starttoken index\n",
        "        # Reshape this to be a (1, 1) array, since it is a sequence of 1 sample for 1 timestep\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "\n",
        "        # Update the target sequence with index of \"start\"\n",
        "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
        "        # Initialize the stop condition with False\n",
        "        stop_condition = False\n",
        "\n",
        "        # Initialize the decoded words with an empty string\n",
        "        decoded_translation = []\n",
        "\n",
        "        # While stop_condition is false\n",
        "        while not stop_condition :\n",
        "\n",
        "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
        "            dec_outputs, h, c, h1, c1 = dec_model.predict([empty_target_seq] + states_values)\n",
        "            # Get the index for sampled word using the dec_outputs\n",
        "            # dec_outputs is a numpy array of the shape (sample, timesteps, VOCAB_SIZE)\n",
        "            # To start, we can just pick the word with the higest probability - greedy search\n",
        "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "            # Initialize the sampled word with None\n",
        "            sampled_word = None\n",
        "\n",
        "            # Iterate through words and their indexes\n",
        "            for word, index in tokenizer.word_index.items() :\n",
        "\n",
        "                # If the index is equal to sampled word's index\n",
        "                if sampled_word_index == index :\n",
        "\n",
        "                    # Add the word to the decoded string\n",
        "                    decoded_translation.append(word)\n",
        "\n",
        "                    # Update the sampled word\n",
        "                    sampled_word = word\n",
        "                    break\n",
        "\n",
        "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
        "            if sampled_word == 'endtoken' or len(decoded_translation) > TEXT_LIMIT:\n",
        "\n",
        "                # Make the stop_condition to true\n",
        "                stop_condition = True\n",
        "\n",
        "            # Initialize back the target sequence to zero - array([[0.]])    \n",
        "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
        "\n",
        "            # Update the target sequence with index of \"start\"\n",
        "            empty_target_seq[0, 0] = sampled_word_index\n",
        "\n",
        "            # Get the state values\n",
        "            states_values = [h, c, h1, c1]\n",
        "\n",
        "            # Print the decoded string\n",
        "        print(' '.join(decoded_translation[:-1]))\n",
        "except KeyboardInterrupt:\n",
        "    print('Ending conversational agent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojquvIdebrnS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}